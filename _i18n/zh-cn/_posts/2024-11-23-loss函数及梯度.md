---
layout: post
title: loss函数及梯度
description:
date: 2024-11-23 19:03:11 +0800
image: https://about.fb.com/wp-content/uploads/2022/09/PyTorch-Foundation-Launch_Header.jpg
tags:
- Algorithm
- Python
- Pytorch
category: ['Algorithm']
---

1. 目录
{:toc}

### 什么是梯度？
梯度是函数在特定点的变化率，表征了在多维空间中函数变化最快的方向。通过梯度，我们能够找到函数的最小值或最大值，在机器学习中尤为关键，因为梯度驱动了模型优化。

**关键概念**：
- **导数（Derivative）**：一维函数的斜率。
- **偏导数（Partial Derivative）**：多维函数在特定变量上的变化率。
- **梯度（Gradient）**：多维函数在所有变量方向上的偏导数向量，指向函数值增长最快的方向。

**代码示例**：
```python
import torch

# 示例：计算简单函数 y = x^2 的梯度
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
y.backward()
print(x.grad)  # 输出梯度：dy/dx = 2x = 4
```

```bash
tensor([4.])
```

梯度下降是一种常用优化算法，其目标是通过更新参数向梯度的负方向移动，从而逐步减小 Loss 函数值。

### 常见函数及其梯度
在机器学习中，模型的预测值通常是输入变量通过参数化函数的输出。常见函数及其梯度的推导是优化的基础。

**函数示例**：
线性函数：$ y = wx + b $
   - 梯度：$ \frac{\partial y}{\partial w} = x ,  \frac{\partial y}{\partial b} = 1 $

二次函数：$ y = (wx + b)^2 $
   - 梯度：$ \frac{\partial y}{\partial w} = 2(wx + b)x $, $ \frac{\partial y}{\partial b} = 2(wx + b) $

**代码示例**：
```python
# 使用 PyTorch 自动求导计算梯度
w = torch.tensor([2.0], requires_grad=True)
b = torch.tensor([1.0], requires_grad=True)
x = torch.tensor([3.0])
y = (w * x + b) ** 2
y.backward()
print(w.grad)  # 输出 w 的梯度
print(b.grad)  # 输出 b 的梯度
```

```bash
tensor([42.])
tensor([14.])
```

### 激活函数及其梯度
激活函数在神经网络中引入非线性。常见的激活函数包括 Sigmoid、Tanh 和 ReLU，其梯度决定了网络的学习能力。

**激活函数及其梯度**：
Sigmoid：
   - 函数：$ \sigma(x) = \frac{1}{1 + e^{-x}} $
   - 梯度：$ \sigma\'(x) = \sigma(x)(1 - \sigma(x)) $

Tanh：
   - 函数：$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $
   - 梯度：$ \tanh\'(x) = 1 - \tanh^2(x) $

ReLU：
   - 函数：$ \text{ReLU}(x) = \max(0, x) $
   - 梯度：$ \text{ReLU}\'(x) = 1 $（当 $ x > 0 $）; $ 0 $（当 $ x \leq 0 $）

**代码示例**：
```python
import torch.nn.functional as F

x = torch.tensor([-1.0, 0.0, 1.0], requires_grad=True)

# 激活函数
y_sigmoid = torch.sigmoid(x)
y_relu = F.relu(x)

# 计算梯度
y_sigmoid.sum().backward()
print(x.grad)  # Sigmoid 梯度
x.grad.zero_()  # 重置梯度
y_relu.sum().backward()
print(x.grad)  # ReLU 梯度
```

```bash
tensor([0.1966, 0.2500, 0.1966])
tensor([0., 0., 1.])
```

### Loss 函数及其梯度
Loss 函数用于衡量模型预测值与真实值之间的差距。常见的 Loss 函数包括均方误差（MSE）和交叉熵（Cross-Entropy）。

**典型 Loss 函数**：
**均方误差（MSE）**：
   - 函数：$ \text{MSE} = \frac{1}{n} \sum (y - \hat{y})^2 $
   - 梯度：$ \frac{\partial \text{MSE}}{\partial \hat{y}} = -2(y - \hat{y}) $

**交叉熵（Cross-Entropy）**：
   - 函数（多分类）：$ \text{CE} = -\sum y \log(\hat{y}) $
   - 梯度：$ \frac{\partial \text{CE}}{\partial \hat{y}} = -\frac{y}{\hat{y}} $

**代码示例**：
```python
# MSE Loss 梯度
y_true = torch.tensor([1.0, 0.0])
y_pred = torch.tensor([0.9, 0.1], requires_grad=True)
loss = torch.nn.functional.mse_loss(y_pred, y_true)
loss.backward()
print(y_pred.grad)  # 输出 MSE 梯度
# 交叉熵 Loss 梯度
y_pred.grad.zero_()
loss = torch.nn.functional.cross_entropy(y_pred, y_true)
loss.backward()
print(y_pred.grad)  # 输出交叉熵梯度
```

```bash
tensor([-0.1000,  0.1000])
tensor([-0.3100,  0.3100])
```

### 结语
在深度学习中，理解梯度和 Loss 函数是优化模型的核心。通过 Python 框架（如 PyTorch）提供的自动求导工具，我们能够高效地计算梯度并更新参数，为构建复杂模型打下坚实基础。