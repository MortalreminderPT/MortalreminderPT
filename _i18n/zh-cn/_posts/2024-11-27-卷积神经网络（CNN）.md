---
layout: post
title: 卷积神经网络（CNN）
description:
date: 2024-11-27 02:12:59 +0800
image: https://about.fb.com/wp-content/uploads/2022/09/PyTorch-Foundation-Launch_Header.jpg
tags:
- Algorithm
- Python
- Pytorch
category: ['Algorithm']
---

1. 目录
{:toc}

## 什么是卷积

卷积是一种数学运算，应用于信号处理和图像分析中，用于提取特征。在深度学习中，卷积层用以提取输入数据的局部特征。

**核心概念：**
1. **卷积核（Kernel/Filter）**：一个小矩阵，用于扫描输入数据。
2. **感受野（Receptive Field）**：卷积核覆盖的区域。
3. **参数共享**：通过共享卷积核的权重减少参数数量，提高效率。

**卷积公式：**
$ (f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d\tau $

**代码示例：**
```python
import torch
import torch.nn as nn

# 定义一个简单的2D卷积
conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)
input_tensor = torch.randn(1, 1, 5, 5)  # 批次大小=1, 通道数=1, 高=5, 宽=5
output = conv(input_tensor)
print("卷积输出大小:", output.shape)
```

1. **nn.Conv2d**：定义一个二维卷积层。
    - in_channels=1：输入的特征图通道数为1（例如灰度图像）。
    - out_channels=1：输出的特征图通道数为1。
    - kernel_size=3：卷积核大小为3x3。
    - stride=1：步幅为1，卷积核每次移动1个像素。
    - padding=1：在输入特征图四周填充1圈0值，确保输出大小与输入一致。
2. **input_tensor**:
    - 模拟一个输入张量（形状为[batch_size, channels, height, width]）。
    - 这里是一个5x5的单通道特征图，批次大小为1。
3. **卷积操作**：
   - `conv(input_tensor)`对输入执行卷积，生成一个输出特征图。
   - 输出大小公式 $
   \text{outputsize} = \frac{\text{inputsize} + 2 × \text{padding} - \text{kernelsize}}{\text{stride}} + 1
   $
   代入参数：$
   \frac{5 + 2 × 1 - 3}{1} + 1 = 5
   $

输出结果将是大小为\[1, 1, 5, 5\]的特征图。


## 卷积神经网络

卷积神经网络（CNN）是一种专门处理网格数据（如图像）的深度学习模型。CNN通过层叠卷积层、激活函数和池化层来学习多层特征表示。

**主要组成部分：**
1. **卷积层（Convolutional Layer）**：用于提取特征。
2. **激活函数（Activation Function）**：如ReLU，用于引入非线性。
3. **全连接层（Fully Connected Layer）**：连接特征到输出。

**代码示例：**
```python
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1) # 卷积层：将输入从1个通道映射到16个通道。
        self.relu = nn.ReLU() # 使用ReLU激活函数：引入非线性特性
        self.fc = nn.Linear(16 * 28 * 28, 10)  # 将16个28x28的特征映射到10类

    def forward(self, x):
        x = self.conv1(x) # 将输入图像卷积为16个特征图
        x = self.relu(x) # 对卷积输出进行非线性变换，帮助模型学习复杂特征
        x = x.view(x.size(0), -1)  # 将二维特征图变为一维向量以输入全连接层
        x = self.fc(x) # 将展平后的特征映射到分类标签
        return x
```

- 输入图像大小为\[28, 28\]，适用于MNIST等数据集。
- 输出为10个分类的分数。



## 池化层，下采样和上采样

### 池化层

池化用于减少特征图尺寸，常见的操作有**最大池化（Max Pooling）**和**平均池化（Average Pooling）**。它们有助于降低计算成本并增加特征不变性。

**代码示例：**
```python
pool = nn.MaxPool2d(kernel_size=2, stride=2)
input_tensor = torch.randn(1, 16, 28, 28)
output = pool(input_tensor)
print("池化后大小:", output.shape) # torch.Size([1, 16, 14, 14])
```

**nn.MaxPool2d**：
- 定义一个最大池化层。
- 参数：
    - `kernel_size=2`：池化窗口为2x2。
    - `stride=2`：窗口每次移动2个像素，防止重叠。

### 下采样和上采样

**下采样（Downsampling）**通过池化或调整分辨率减少特征图尺寸。

**上采样（Upsampling）**用于增大特征图尺寸，常用于生成模型和语义分割。

**代码示例：**
```python
import torch.nn.functional as F

# 下采样
downsampled = F.interpolate(input_tensor, scale_factor=0.5, mode='bilinear')

# 上采样
upsampled = F.interpolate(input_tensor, scale_factor=2, mode='bilinear')

print("下采样后大小:", downsampled.shape)
print("上采样后大小:", upsampled.shape)
```

1. **下采样**：
   - `scale_factor=0.5`：将特征图尺寸缩小一半。
   - `mode='bilinear'`：使用双线性插值，适合连续值特征。

2. **上采样**：
   - `scale_factor=2`：将特征图尺寸扩大两倍。
   - `mode='bilinear'`：平滑地填充像素值。

---

## BatchNorm

批归一化（Batch Normalization）是用于加速模型收敛并提高性能的技术。它通过调整激活分布的均值和标准差，使模型更稳定，允许使用更大的学习率。

**主要优点：**
1. 加速收敛。
2. 降低过拟合。
3. 提高模型稳定性。

**代码示例：**
```python
x = torch.randn(1, 16, 7, 7)
print("x.shape =", x.shape)
layer = nn.BatchNorm2d(num_features=16)
out = layer(x)
print("out.shape =", out.shape)
print("layer.weight.shape =", layer.weight.shape)
print("layer.bias.shape =", layer.bias.shape)
```

```bash
x.shape = torch.Size([1, 16, 7, 7])
out.shape = torch.Size([1, 16, 7, 7])
layer.weight.shape = torch.Size([16])
layer.bias.shape = torch.Size([16])
vars(layer) = {
	'training': True,
	'_parameters': OrderedDict([('weight', Parameter containing:
		tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
			requires_grad = True)), ('bias', Parameter containing:
		tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
			requires_grad = True))]),
	'_buffers': OrderedDict([('running_mean', tensor([0.0283, 0.0018, -0.0207, -0.0116, -0.0092, -0.0127, 0.0054, -0.0146, -0.0152, -0.0171, -0.0153, 0.0133, 0.0122, -0.0066, 0.0116, 0.0064])), ('running_var', tensor([0.9926, 0.9942, 0.9708, 0.9982, 1.0170, 0.9690, 1.0005, 1.0011, 0.9939,
		0.9892, 0.9577, 0.9949, 1.0047, 0.9766, 0.9797, 1.0248
	])), ('num_batches_tracked', tensor(1))]),
	'_non_persistent_buffers_set': set(),
	'_backward_hooks': OrderedDict(),
	'_is_full_backward_hook': None,
	'_forward_hooks': OrderedDict(),
	'_forward_pre_hooks': OrderedDict(),
	'_state_dict_hooks': OrderedDict(),
	'_load_state_dict_pre_hooks': OrderedDict(),
	'_load_state_dict_post_hooks': OrderedDict(),
	'_modules': OrderedDict(),
	'num_features': 16,
	'eps': 1e-05,
	'momentum': 0.1,
	'affine': True,
	'track_running_stats': True
}
```

1. **`nn.BatchNorm2d`**：
   - 定义一个批归一化层，用于二维特征图。
   - 参数：
     - `num_features=16`：特征图的通道数。

2. **输入张量**：
   - 模拟一个批次大小为1，通道数为16，大小为28x28的特征图。

3. **批归一化操作**：
   - 将每个通道的特征标准化为均值为0，方差为1的分布。
   - 使用可训练参数重新缩放和偏移：
     $
     y = \gamma \cdot \frac{x - \mu}{\sigma} + \beta
     $
   - 有助于模型在训练时加速收敛并提高稳定性。