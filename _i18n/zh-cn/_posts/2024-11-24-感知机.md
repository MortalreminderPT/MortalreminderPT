---
layout: post
title: 感知机
description:
date: 2024-11-24 09:32:17 +0800
image: https://about.fb.com/wp-content/uploads/2022/09/PyTorch-Foundation-Launch_Header.jpg
tags:
- Algorithm
- Python
- Pytorch
category: ['Algorithm']
---

1. 目录
{:toc}

### 感知机原理及实现

感知机是机器学习中的基础概念，也是神经网络的基石。本文将通过单一输出感知机、多输出感知机、链式法则以及反向传播算法，深入探讨感知机的原理和实现。

---

#### 单一输出感知机

单一输出感知机是最简单的神经网络。其核心是利用一个线性方程 $ y = XW + b $ 将输入映射到输出。

##### 数学公式

- 激活函数：$
y = \sigma(\mathbf{X} \cdot \mathbf{W} + b)
$
  其中 $\sigma$ 为激活函数（通常是Sigmoid函数）。
- 损失函数（平方误差）：$
E = \frac{1}{2}(O - t)^2
$

##### 梯度计算

通过梯度下降优化感知机的权重，计算过程如下：
$
\frac{\partial E}{\partial w_j} = (O - t) \cdot O \cdot (1 - O) \cdot x_j
$

##### Python代码示例

```python
import numpy as np

# 激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 前向传播
def forward(X, W, b):
    return sigmoid(np.dot(X, W) + b)

# 损失函数
def loss(O, T):
    return 0.5 * np.sum((O - T) ** 2)

# 梯度更新
def update_weights(X, W, b, O, T, lr=0.1):
    delta = (O - T) * O * (1 - O)
    dW = np.dot(X.T, delta)
    db = np.sum(delta)
    W -= lr * dW
    b -= lr * db
    return W, b


X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

# 目标输出
T = np.array([[0], [0], [0], [1]])

# 初始化权重和偏置
np.random.seed(42)
W = np.random.randn(2, 1)
b = np.random.randn(1)

# 训练参数
epochs = 10000
lr = 0.1

O = forward(X, W, b)
predictions = (O > 0.5).astype(int)
print("\n训练前的输出:")
print(predictions)

# 训练过程
for epoch in range(epochs):
    O = forward(X, W, b)
    current_loss = loss(O, T)
    W, b = update_weights(X, W, b, O, T, lr)
    if epoch % 2000 == 0:
        print(f"Epoch {epoch}, Loss: {current_loss}")

# 测试结果
O = forward(X, W, b)
predictions = (O > 0.5).astype(int)
print("\n训练后的输出:")
print(predictions)
```

```bash
训练前的输出:
[[1]
 [1]
 [1]
 [1]]
Epoch 0, Loss: 0.7340890587930462
Epoch 2000, Loss: 0.031960823202117496
Epoch 4000, Loss: 0.01475373398032874
Epoch 6000, Loss: 0.009359312487717476
Epoch 8000, Loss: 0.006791321484745856
训练后的输出:
[[0]
 [0]
 [0]
 [1]]
```
---

#### 多输出感知机

多输出感知机扩展了单一输出的模型，允许多个输出节点，用于多分类任务。

##### 数学公式

- 多输出公式：$
y_k = \sigma(\mathbf{X} \cdot \mathbf{W_k} + b_k)
$
- 总误差：$
E = \frac{1}{2} \sum_k (O_k - t_k)^2
$

##### 梯度计算

$
\frac{\partial E}{\partial w_{jk}} = (O_k - t_k) \cdot O_k \cdot (1 - O_k) \cdot x_j
$

##### Python代码示例

```python
import numpy as np
from sklearn import datasets
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 前向传播
def forward_multi(X, W, b):
    return sigmoid(np.dot(X, W) + b)

# 交叉熵损失函数
def loss_multi(O, T):
    epsilon = 1e-15
    O = np.clip(O, epsilon, 1 - epsilon)
    return -np.sum(T * np.log(O)) / O.shape[0]

# 多输出梯度更新（修正后的）
def update_weights_multi(X, W, b, O, T, lr=0.1):
    delta = (O - T) * O * (1 - O)
    dW = np.dot(X.T, delta)
    db = np.sum(delta, axis=0)
    W -= lr * dW
    b -= lr * db
    return W, b

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data  # 特征
y = iris.target.reshape(-1, 1)  # 类别

# One-Hot 编码
encoder = OneHotEncoder(sparse=False)
T = encoder.fit_transform(y)

# 数据标准化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.2, random_state=42)

# 初始化权重和偏置
np.random.seed(42)
input_dim = X_train.shape[1]
output_dim = T_train.shape[1]
W = np.random.randn(input_dim, output_dim)
b = np.random.randn(output_dim)

# 训练参数
epochs = 10000
lr = 0.1

# 训练过程
for epoch in range(epochs):
    O = forward_multi(X_train, W, b)
    current_loss = loss_multi(O, T_train)
    W, b = update_weights_multi(X_train, W, b, O, T_train, lr)
    if epoch % 2000 == 0:
        print(f"Epoch {epoch}, Loss: {current_loss}")

# 测试结果
O_test = forward_multi(X_test, W, b)
predictions = np.argmax(O_test, axis=1)
true_labels = np.argmax(T_test, axis=1)
accuracy = np.mean(predictions == true_labels)
print(f"\n测试集准确率: {accuracy * 100:.2f}%")
```

```bash
Epoch 0, Loss: 2.0055848558408393
Epoch 2000, Loss: 0.2695123992451301
Epoch 4000, Loss: 0.26755760116702415
Epoch 6000, Loss: 0.2668457531681846
Epoch 8000, Loss: 0.26653101686497677

测试集准确率: 100.00%
```

---

#### 链式法则

链式法则是计算复杂神经网络梯度的核心工具，它将误差通过网络的各层逐层传播。

##### 链式法则公式

$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \cdot \frac{\partial u}{\partial x}
$

在神经网络中：
$
\frac{\partial E}{\partial w_j} = \frac{\partial E}{\partial O_k} \cdot \frac{\partial O_k}{\partial x} \cdot \frac{\partial x}{\partial w_j}
$

##### Python代码示例

```python
# 链式法则梯度传播
def chain_rule_grad(X, W, b, T, lr=0.1):
    O = forward(X, W, b)
    delta = (O - T) * O * (1 - O)  # 误差传播
    dW = np.dot(X.T, delta)
    db = np.sum(delta, axis=0)
    W -= lr * dW
    b -= lr * db
    return W, b
```

---

#### 反向传播算法

反向传播（Backpropagation）将链式法则应用于多层神经网络中。

##### 公式推导

- 输出层：
$
\delta_k = (O_k - t_k) \cdot O_k \cdot (1 - O_k)
$
- 隐藏层：
$
\delta_j = (\sum_k \delta_k \cdot w_{jk}) \cdot O_j \cdot (1 - O_j)
$

##### Python代码示例

```python
import numpy as np

# 激活函数及其导数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

# 反向传播函数
def backprop(X, W1, b1, W2, b2, T, lr=0.1):
    # 前向传播
    Z1 = np.dot(X, W1) + b1
    H = sigmoid(Z1)
    Z2 = np.dot(H, W2) + b2
    O = sigmoid(Z2)

    # 反向传播
    delta_output = (O - T) * sigmoid_derivative(Z2)
    delta_hidden = np.dot(delta_output, W2.T) * sigmoid_derivative(Z1)

    # 梯度更新
    W2 -= lr * np.dot(H.T, delta_output)
    b2 -= lr * np.sum(delta_output, axis=0)
    W1 -= lr * np.dot(X.T, delta_hidden)
    b1 -= lr * np.sum(delta_hidden, axis=0)

    # 计算损失
    loss = 0.5 * np.sum((O - T) ** 2)
    return W1, b1, W2, b2, loss

# 数据集：XOR 问题
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

T = np.array([
    [0],
    [1],
    [1],
    [0]
])

# 初始化权重和偏置
np.random.seed(42)
input_dim = X.shape[1]
hidden_dim = 2  # 隐藏层神经元数量
output_dim = 1

W1 = np.random.randn(input_dim, hidden_dim)
b1 = np.random.randn(hidden_dim)
W2 = np.random.randn(hidden_dim, output_dim)
b2 = np.random.randn(output_dim)

# 训练参数
epochs = 10000
lr = 0.1

# 训练过程
for epoch in range(epochs):
    W1, b1, W2, b2, current_loss = backprop(X, W1, b1, W2, b2, T, lr)
    if epoch % 2000 == 0:
        print(f"Epoch {epoch}, Loss: {current_loss}")

# 测试结果
Z1 = np.dot(X, W1) + b1
H = sigmoid(Z1)
Z2 = np.dot(H, W2) + b2
O = sigmoid(Z2)
predictions = (O > 0.5).astype(int)
print("\n训练后的输出:")
print(predictions)
```

```bash
Epoch 0, Loss: 0.5887016577000074
Epoch 2000, Loss: 0.4070781963562403
Epoch 4000, Loss: 0.09266207200904633
Epoch 6000, Loss: 0.016896752153564412
Epoch 8000, Loss: 0.008292971056384607

训练后的输出:
[[0]
 [1]
 [1]
 [0]]
```