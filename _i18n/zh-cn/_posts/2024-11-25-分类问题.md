---
layout: post
title: 分类问题
description:
date: 2024-11-25 05:17:42 +0800
image: https://about.fb.com/wp-content/uploads/2022/09/PyTorch-Foundation-Launch_Header.jpg
tags:
- Algorithm
- Python
- Pytorch
category: ['Algorithm']
---

1. 目录
{:toc}

### 使用 PyTorch 进行分类问题

分类问题是机器学习中的核心任务之一，利用 PyTorch，我们可以实现从逻辑回归到多层神经网络的完整解决方案。本文将从以下六个方面系统讲解 PyTorch 在分类问题中的应用：

---

#### 1. 逻辑回归

逻辑回归是解决二分类问题的基础工具。其核心是通过逻辑函数（Sigmoid）将线性回归的输出映射到 $[0, 1]$ 区间，表示概率值。

**核心公式**：
- 连续值预测：$ y = xw + b $
- 概率输出：$ y = \sigma(xw + b) $，其中 $ \sigma(x) $ 是 Sigmoid 函数。

**实现要点**：
- 输出值通过 Sigmoid 激活函数。
- 损失函数通常选用二分类交叉熵。
- criterion 不需要对 label 做 one-hot，会在计算时自动完成。
- forward 不需要对计算结果做 Softmax，会在计算时自动添加。

**代码示例**：
```python
import torch
import torch.nn as nn
import torch.optim as optim

# 数据准备
x_data = torch.tensor([[1.0], [2.0], [3.0]], dtype=torch.float32)
y_data = torch.tensor([[0.0], [0.0], [1.0]], dtype=torch.float32)

# 定义逻辑回归模型
class LogisticRegressionModel(nn.Module):
    def __init__(self):
        super(LogisticRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)  # 输入特征1，输出特征1

    def forward(self, x):
        return torch.sigmoid(self.linear(x))

model = LogisticRegressionModel()
criterion = nn.BCELoss()  # 二分类交叉熵
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 训练过程
for epoch in range(10000):
    y_pred = model(x_data)  # 模型预测
    loss = criterion(y_pred, y_data)  # 计算损失
    optimizer.zero_grad()  # 梯度清零
    loss.backward()  # 反向传播
    optimizer.step()  # 更新参数

    # 每100个epoch输出一次日志
    if (epoch + 1) % 2000 == 0:
        print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")

# 打印训练结束后的参数
print("Training complete.")
print("Model parameters:")
for name, param in model.named_parameters():
    print(f"{name}: {param.data}")

# 测试模型
with torch.no_grad():
    test_data = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)
    predictions = model(test_data)
    print("Test Predictions:")
    for i, pred in enumerate(predictions):
        print(f"Input: {test_data[i].item()}, Predicted Probability: {pred.item():.4f}, Predicted Class: {1 if pred.item() >= 0.5 else 0}")
```

```bash
Epoch 2000, Loss: 0.3870
Epoch 4000, Loss: 0.2911
Epoch 6000, Loss: 0.2380
Epoch 8000, Loss: 0.2034
Epoch 10000, Loss: 0.1786
Training complete.
Model parameters:
linear.weight: tensor([[2.5076]])
linear.bias: tensor([-6.0840])
Test Predictions:
Input: 1.0, Predicted Probability: 0.0272, Predicted Class: 0
Input: 2.0, Predicted Probability: 0.2556, Predicted Class: 0
Input: 3.0, Predicted Probability: 0.8083, Predicted Class: 1
Input: 4.0, Predicted Probability: 0.9810, Predicted Class: 1
```

---

#### 2. 交叉熵

交叉熵是分类问题的常用损失函数，特别适用于概率分布的比较。

**核心概念**：
- 衡量两个概率分布的差异。
- 对于一热编码：$ H(p, q) = -\sum p(x) \log q(x) $。

**与 MSE 比较**：
- MSE 在分类问题中容易导致梯度消失。
- 交叉熵更适合梯度优化。

**代码示例**：
```python
criterion = nn.CrossEntropyLoss()
# 输出 logits，而非 softmax 激活后的值
outputs = model(inputs)
loss = criterion(outputs, labels)
```

---

#### 3. 多分类问题

多分类问题扩展了二分类问题，模型需要预测 $ N $ 个类别的概率。

**核心步骤**：
1. 使用 Softmax 将模型输出转为概率分布。
2. 选择交叉熵损失函数。

**代码示例**：
```python
import torch.nn.functional as F

class MultiClassModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(MultiClassModel, self).__init__()
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        return self.fc(x)  # 不需要 softmax ，因为 CrossEntropyLoss 期望接收的是原始 logits 而不是概率分布。

model = MultiClassModel(input_dim=10, num_classes=3)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

---

#### 4. 全连接层

全连接层是神经网络的核心模块，用于将输入特征映射到输出空间。

**实现方式**：
- 通过 `nn.Linear` 定义全连接层。
- 在 `forward` 函数中调用层对象。

**代码示例**：
```python
class FullyConnectedModel(nn.Module):
    def __init__(self):
        super(FullyConnectedModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))  # 激活函数选择
        x = self.fc2(x)
        return x
```

---

#### 5. 激活函数选择

激活函数为神经网络提供非线性变换，以下是常用激活函数：
- **ReLU**：高效且收敛快，但容易出现“神经元死亡”问题。
- **Leaky ReLU**：改进了 ReLU 的问题。
- **Softplus**：平滑的 ReLU 替代。
- **SELU**：适用于特殊归一化条件。

**代码对比**：
```python
x = torch.tensor([-1.0, 0.0, 1.0])
relu = F.relu(x)
leaky_relu = F.leaky_relu(x)
softplus = F.softplus(x)
```

---

#### 6. MNIST 实战

MNIST 数据集是图像分类的经典任务，用于识别手写数字。

**核心流程**：
1. 加载数据集。
2. 定义模型（包含全连接层和激活函数）。
3. 训练与测试。

**代码示例**：
```python
# 数据预处理和加载
transform = transforms.ToTensor()
train_loader = DataLoader(datasets.MNIST('.', train=True, download=True, transform=transform), batch_size=64, shuffle=True)
test_loader = DataLoader(datasets.MNIST('.', train=False, transform=transform), batch_size=1000)

# 初始化设备、模型、损失函数和优化器
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FullyConnectedModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
for epoch in range(5):
    model.train()
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# 测试模型
model.eval()
correct = 0
with torch.no_grad():
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        output = model(data)
        pred = output.argmax(dim=1)
        correct += (pred == target).sum().item()
print(f"测试准确率: {100. * correct / len(test_loader.dataset):.2f}%")
```

```bash
Epoch 1, Loss: 0.1609
Epoch 2, Loss: 0.1509
Epoch 3, Loss: 0.0351
Epoch 4, Loss: 0.1062
Epoch 5, Loss: 0.0408
测试准确率: 97.61%
```